{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        Tony Chen\n",
    "        1005994872\n",
    "        CSCC11\n",
    "        Dr. Masoud Ataei\n",
    "        December 21, 2021  \n",
    "```\n",
    "  \n",
    "    \n",
    "    \n",
    "Firstly, in this report, we take a close look at the CBOE volatility index (VIX) and fit a mixture of gaussians model in order to understand the some of the underlying phenomenon of why the market behaves the way it does.  \n",
    "\n",
    "Secondly, we attempt to understand some of the forces that are driving the VIX index.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Although it certainly is not possible to predict and forecast the stock fear market with a perfect degree of accuracy, it still may very well be worth the time to figure the underlying phenomenon(s) that drive and influence it. If some underlying phenomenon can be discovered for the behaviour of the fear market, it can be used to predict periods of high volatility in the market or periods where the market is unstable. If one can predict these behaviours of the market like volatility to some degree of accuracy then that information will be very helpful in determining when it is a good time to be in the market and when it is probably better to leave the market based on risk.  \n",
    "\n",
    "Predicting the behaviour of the stock market is an extremely difficult task because there is an endless number of factors that can influence the stock market. Political tensions, natural resources, virus outbreaks, essentially anything that could show up on the news and cause panic or excitement could impact the stock market. However in light of these many unpredictable factors, in order to be practical we will not look at every single possible factor to simplify the problem of finding some underlying phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the VIX Index\n",
    "**Q1.1**  \n",
    "The first step was to pre-process the data of the VIX dataset to give the monthly log returns. The code for this can be found in [Appendix A: Loading the MVIX data]  \n",
    "\n",
    "The next step was to discretize the data. In order to discretize the data, a histogram was used. From the minimum monthly log return to the maximum monthly log return, the data was organized into *48* equally sized bins. After vizualing and trying out various number of bins, anywhere from 40-50 bins produced a reasonable organization of bins.  \n",
    "\n",
    "Having too little bins resulted in most of the observed data falling into a few bins which does not provide a useful distribution of data.  \n",
    "Having too many bins resulted in a lot of bins having very low frequency or just not having any data points at all.  \n",
    "\n",
    "40-50 bins in general, minimized the above problems. 48 was chosen for the number of bins for this study.  \n",
    "The code for this discretization of the data along with a histogram can be found in [Appendix B: Discretization]  \n",
    "\n",
    "The next goal was to model a mixture of Gaussian distribution to the discretized data and determine what is the optimal number of regimes the model should have.  \n",
    "\n",
    "The implementation of the objective function for the GA solver to minimize can be found in [Appendix C: Objective function for GA Solver]. The objective value is the one defined in the provided Q1 Question sheet denoted by $l(\\theta)$.  \n",
    "\n",
    "The goal is to maximize the log-likelihood for the mixture of Gaussian distribution over the discretized observed monthly log returns.  \n",
    "\n",
    "In this problem we have the following constraints:  \n",
    "$\\sum_{i=1}^{k} \\pi_{i} = 1$  \n",
    "$\\pi_{i} \\in [0,1]$ &emsp;&emsp;&emsp;&nbsp;&nbsp; $i=1,...,k$  \n",
    "$\\mu_{i} \\in (-\\infty,\\infty)$ &nbsp;&nbsp;&nbsp;&nbsp; $i=1,...,k$  \n",
    "$\\sigma_{i} \\in (0,\\infty)$ &emsp;&emsp;&nbsp;&nbsp; $i=1,...,k$  \n",
    "\n",
    "Directly modeling $\\sum_{i=1}^{k} \\pi_{i} = 1$ is extremely hard so in order to model this constraint, a lagrangian relaxation was used and introduced into the objective value.  \n",
    "The relaxed objective value became $l(\\theta) + \\lambda(|\\sum_{i=1}^{k}\\pi_{i} - 1|)$  \n",
    "By introducing the lagrangian, the model is penalized if the sum of the probability mixtures is not equal to 1.  \n",
    "Ofcourse, this lagrangian introduces the lagrangian multiplier $\\lambda$ parameter to also learn. After many tests, a lower bound on $\\lambda$ of $1000$ was appropriate to make sure that the model satisfied the constraint.  \n",
    "\n",
    "The bound for all of the probability mixtures $\\pi_{i}$ was set to $[0.05, 1]$. This is because a value of 0 means that one of the regimes vanished which essentially results in not actually having k regimes. A lower bound of $0.05$ was set to prevent this behaviour.  \n",
    "\n",
    "To model $\\mu_{i} \\in (-\\infty,\\infty)$, a bound of $[-1, 1]$ was appropriate as looking at the histogram in [Appendix B: Discretization], the mean of any regime should definitely not be outside this bound.  \n",
    "\n",
    "To model $\\sigma_{i} \\in (0,\\infty)$, a bound of $[0.0001, 5]$ was used as it is clearly observable from the histogram that no standard deviation should be greater than 5 and a lower bound of 0.0001 is well appropriate as it is extremely unlikely to have such little variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many runs of the GA solver for the number of regimes ranging from k=1 to 5, the results can be summarized into the following table below where k represents the number of regimes of the MoG model.  \n",
    "The code for computing the objective value $l(\\theta^{*})$ can be found in [Appendix D: Objective value Without Lagrangian]  \n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "**Q1.2**\n",
    "```\n",
    "|     k      |       1      |       2      |        3     |      4       |       5      |  \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|    \n",
    "|   pi_1     | 9.99981e-01  | 4.56548e-01  | 6.08524e-02  | 1.32297e-01  | 3.68164e-01  |    \n",
    "|   mu_1     | -1.88338e-03 | -4.63689e-02 | -4.75877e-02 | 3.71483e-03  | -2.67953e-02 |  \n",
    "|   sigma_1  | 1.84790e-01  | 1.117996-01  | 5.61458e-02  | 2.73503e-01  | 9.31952e-02  |   \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|   \n",
    "|   pi_2     |              | 5.43460e-01  | 1.50573e-01  | 2.07789e-01  | 6.13773e-02  |   \n",
    "|   mu_2     |              | 3.68542e-02  | 1.25448e-01  | -8.04863e-02 | 1.42801e-02  |   \n",
    "|   sigma_2  |              | 2.21456e-01  | 2.44038e-01  | 1.07804e-01  | 1.92861e-01  |   \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|       \n",
    "|   pi_3     |              |              | 7.88531e-01  | 8.23921e-02  | 4.08659e-01  |   \n",
    "|   mu_3     |              |              | -2.16210e-02 | 2.22703e-01  | 2.50715e-02  |   \n",
    "|   sigma_3  |              |              | 1.61205e-01  | 1.51214e-01  | 2.40994e-01  |   \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|       \n",
    "|   pi_4     |              |              |              | 5.77850e-01  | 5.07813e-02  |   \n",
    "|   mu_4     |              |              |              | -1.27775e-02 | -7.53068e-02 |   \n",
    "|   sigma_4  |              |              |              | 1.48975e-01  | 1.95623e-01  |   \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|       \n",
    "|   pi_5     |              |              |              |              | 1.11934e-01  |   \n",
    "|   mu_5     |              |              |              |              | -7.68256e-02 |   \n",
    "|   sigma_5  |              |              |              |              | 3.23133e-01  |   \n",
    "|------------|--------------|--------------|--------------|--------------|--------------|       \n",
    "|   chi^2    | 63.509964849 | 45.957909726 | 45.231243580 | 47.171214578 | 57.318537929 |\n",
    "```       \n",
    "\n",
    "**Q1.3**  \n",
    "In order to decide the optimal number of components of the mixture distribution, an intuitive deterministic approach was taken where the optimal number of components was taken based on the model which returned the lowest $l(\\theta^{*})$. A low value of $l(\\theta^{*})$ would indicate that the theoretical MoG model fits the data well. In this case, k=3 regimes presented the most ideal model.  \n",
    "\n",
    "**Q1.4**  \n",
    "Before the probability transition matrix is computed, the regime present at any given month must first be defined.  \n",
    "The regime present at any given month is that with the maximum conditional probability (mcp)\n",
    "\n",
    "$$ \\arg \\max_{i}\\{P(S_{t} = i | x_{t};\\theta)\\} $$\n",
    "$P(S_{t} = i | x_{t};\\theta)$ is the same as the one denoted on the Q1 handout.  \n",
    "The code for computing the regimes present at each month of the dataset along with the output can be found at [Appendix E: Regime Present at Any Given Month]  \n",
    "\n",
    "From observing the list of regimes present at each month, it turns out that regime 1 never appears. From the mixture model we generated above with $k=3$ regimes, this behaviour can be explained by the fact that regime 1 has an extremely low probability mixture of $\\pi_{1}=0.0608524264$  \n",
    "So for this model with 3 regimes with this specific dataset, essentially only 2 regimes are significant. This behaviour is slightly justified given the mixture model with $k=2$ regimes also produced an almost equally low objective value meaning that having a model with 2 regimes would also make sense.  \n",
    "\n",
    "Given this, our transition matrix will be a 2x2 matrix describing the probability by which the market may switch from regime 2 and 3 accordingly.  \n",
    "\n",
    "After computing the regime present at each month, the probability transition matrix is described in the table below:  \n",
    "```\n",
    "|          |  Regime 2  |  Regime 3  |\n",
    "|----------|------------|------------|\n",
    "| Regime 2 |    0.05    |    0.95    |  \n",
    "| Regime 3 | 0.05621302 | 0.94378698 |\n",
    "```\n",
    "\n",
    "Where the matrix at position (regime i, regime j) represents the probability of switching to regime j given the present regime is i.  \n",
    "The code for computing the transition matrix can be found at [Appendix F: Transition Matrix].  \n",
    "\n",
    "Observing this table, it is quickly evident that one of the regimes (Regime 3) has a much higher probability of being present at any given month. If one takes a look at a historical chart of the VIX index price, this transition matrix does make sense. The VIX index mostly hovers around a stable price representing the average implied volatility of the market. This behaviour can be represented as Regime 3 which is present most of the time. However, one can also see that the VIX price will also jump dramatically at rare times meaning that the implied volatility of the market occasionally becomes high and this behaviour can be represented as Regime 2 in our model.  \n",
    "\n",
    "**Q1.5**  \n",
    "When taking a look at a graph of the S&P 500 index which tracks the performance of the 500 largest companies listed on the US stock exchanges, one can identify 2 states of the index; a steady rise in the price and dips where the index falls. These two states of the index can represent our 2 significant regimes of our model. When the market is rising, the market is said to be bullish. On the other hand, when the market is falling, it is said to be bearish. We can see that most of the time, the market is rising (bullish) and every once in a while the market will fall (bearish) similar to how our model describes regime 3 to be present most of the time, but also having regime 2 being present every once in a while. This switching of the market from being bullish to bearish can represent the regime-switching phenomenon.  \n",
    "\n",
    "As to why this phenomenon occurs, there can be many reasons. In a normal state, the market can be seen to be rising most of the time however there are also sudden market crashes that can be observed.  \n",
    "The dot-com bubble was a period in the year 2000 were the market had a sudden significant drop. This dot-com bubble market crash is said to be the cause of investors being too excited about the future of the internet and investing into any company ending with a .com which caused valuations to rise way beyond what most of these companies were actually worth. Then of course, reality has to kick in at some point in which the stock market took a huge dip causing a lot of these new internet companies at the time to completely disappear.  \n",
    "Another example is the 2008 U.S. housing market crash which was caused by investors buying up bonds called mortgage-backed securities which in hind-sight was just investors not wanting to miss out on the money being generated without realizing how terrible these securities truly were.  \n",
    "Most recently, the stock market took a dip in 2020 when fears of the covid-19 pandemic started to settle in.  \n",
    "As we can observe, the stock market is steadily rising in a normal state/regime but every once in a while, the market will switch states/regimes and fall due to reasons such as overly optimistic valuations, epidemics, inflation, etc.  \n",
    "Ultimately, I believe that the regime-switching phenomenon occurs when real-world events cause investors to develop different emotions about the future directions of the market whether the emotion be confidence or fear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Equity Market Volatility (EMV) Tracker\n",
    "**Q2**  \n",
    "The code for loading the MEMV.pkl dataset can be found at [Appendix G: Loading the EMV Dataset]  \n",
    "\n",
    "The following are the results of building OLS, LASSO, Ridge, and Elastic Net Regression models using 5-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression\n",
      "5-fold Cross-Validation Scores (Coefficient of determination R^2):\n",
      "[-0.80217055 -0.90962688  0.15949264  0.36054169 -9.30769362]\n",
      "\n",
      "Coefficient vector\n",
      "[-6.53455228e-02  8.77354485e-01  1.26507836e-01  7.70003491e-01\n",
      "  2.12780638e-01  1.11178858e-01 -1.23471449e+00 -8.36931928e-01\n",
      "  6.29076956e-02 -2.06953545e-01 -8.80297373e-01  1.27476895e+00\n",
      "  6.83654398e-02  6.48800444e-01  1.81586822e-01 -3.40122234e-02\n",
      " -2.71574615e+00 -1.28178079e+00  4.24711035e-02  3.63147510e+00\n",
      "  3.12975411e+00  6.04637064e-01 -7.08040554e-01 -1.20331804e+00\n",
      " -6.41232232e-02 -6.03658780e-01 -1.24472454e+00  1.61098275e+00\n",
      "  7.28211815e-01 -1.45762367e+00  7.04797310e-01 -2.93781512e-01\n",
      "  3.68273594e+00  5.67565606e-01  9.93953048e-01 -1.72836798e+00\n",
      " -3.54075388e-01  4.97844656e-01 -6.60790771e-01  1.65104028e+00\n",
      "  3.05628541e+00 -3.82721540e-01  2.70257311e-03 -8.91079954e+00\n",
      " -3.91077529e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "olsReg = LinearRegression().fit(X, y)\n",
    "\n",
    "print('OLS Regression')\n",
    "print('5-fold Cross-Validation Scores (Coefficient of determination R^2):')\n",
    "print(cross_val_score(olsReg, X, y, cv=5))\n",
    "print('\\nCoefficient vector')\n",
    "print(olsReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO Regression\n",
      "5-fold Cross-Validation Scores (Coefficient of determination R^2):\n",
      "[-0.01658885 -0.77808529  0.41213556  0.54725391  0.17460342]\n",
      "\n",
      "Penalty term multiplier:  1.2019255817330834\n",
      "\n",
      "Coefficient vector\n",
      "[ 0.76665102  0.         -0.03637395  0.          0.          0.\n",
      " -0.80774182  0.         -0.          0.07726687 -0.         -0.\n",
      "  0.          0.         -0.          0.         -0.         -0.\n",
      "  0.          0.          0.         -0.          0.         -0.\n",
      " -0.         -0.02588741  0.          0.19385272  0.          0.\n",
      " -0.         -0.          0.         -0.          0.          0.\n",
      "  0.          0.         -0.         -0.          0.          0.\n",
      " -0.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lassoReg = LassoCV(cv=5).fit(X, y)\n",
    "\n",
    "print('LASSO Regression')\n",
    "print('5-fold Cross-Validation Scores (Coefficient of determination R^2):')\n",
    "print(cross_val_score(lassoReg, X, y, cv=5))\n",
    "print('\\nPenalty term multiplier: ', lassoReg.alpha_)\n",
    "print('\\nCoefficient vector')\n",
    "print(lassoReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression\n",
      "5-fold Cross-Validation Scores (Coefficient of determination R^2):\n",
      "[-0.47458211 -0.76916987  0.25860034  0.51529483 -5.34435011]\n",
      "\n",
      "Penalty term multiplier:  10.0\n",
      "\n",
      "Coefficient vector\n",
      "[ 1.94591731e-01  6.01130905e-01  6.21501336e-02  5.04945810e-01\n",
      "  3.09455147e-01  1.57861744e-01 -1.18930355e+00 -5.16737238e-01\n",
      "  4.05996401e-02 -1.56717718e-01 -7.42282173e-01  5.62979867e-01\n",
      "  2.13564599e-01  4.34102611e-01  1.06927865e-01 -2.25685866e-02\n",
      " -1.35776342e+00 -7.58630629e-01  8.92193552e-02  2.85642746e+00\n",
      "  2.00833071e+00  2.52370470e-01 -3.50970675e-01 -8.89106963e-01\n",
      "  1.12014655e-01 -5.33131009e-01 -8.53720520e-01  1.33009725e+00\n",
      "  7.53705873e-01  6.64303008e-02  4.98934699e-01  4.10394128e-02\n",
      "  1.85736252e+00 -1.19580624e-01  4.21722564e-01 -7.21610655e-01\n",
      " -2.84106908e-01  5.26748072e-01 -5.30805971e-01  2.88866161e-01\n",
      "  1.08086998e+00 -1.05965710e-01 -2.24592706e-03 -1.12330647e+00\n",
      " -3.00584840e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridgeReg = RidgeCV(cv=5).fit(X, y)\n",
    "print('Ridge Regression')\n",
    "print('5-fold Cross-Validation Scores (Coefficient of determination R^2):')\n",
    "print(cross_val_score(ridgeReg, X, y, cv=5))\n",
    "print('\\nPenalty term multiplier: ', ridgeReg.alpha_)\n",
    "print('\\nCoefficient vector')\n",
    "print(ridgeReg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Regression\n",
      "5-fold Cross-Validation Scores (Coefficient of determination R^2):\n",
      "[-0.06768307 -0.6513637   0.56274517  0.53944655  0.17755212]\n",
      "\n",
      "L1 ratio:  0.6\n",
      "Penalty term multiplier:  2\n",
      "\n",
      "Coefficient vector\n",
      "[ 6.88462552e-01  0.00000000e+00 -5.57294774e-04  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00 -5.70739923e-01  0.00000000e+00\n",
      " -0.00000000e+00  1.24325032e-01 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -7.00552800e-02  0.00000000e+00  2.57460413e-01\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "elasticNetReg = ElasticNetCV(l1_ratio=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], alphas=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], cv=5).fit(X, y)\n",
    "print('Elastic Net Regression')\n",
    "print('5-fold Cross-Validation Scores (Coefficient of determination R^2):')\n",
    "print(cross_val_score(elasticNetReg, X, y, cv=5))\n",
    "print('\\nL1 ratio: ', elasticNetReg.l1_ratio_)\n",
    "print('Penalty term multiplier: ', elasticNetReg.alpha_)\n",
    "print('\\nCoefficient vector')\n",
    "print(elasticNetReg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing and comparing the results of OLS, LASSO, Ridge, and Elastic Net Regression, the following can be interpreted from each regression model.  \n",
    "\n",
    "$$L_{OLS} = ||y - Xw||^{2}$$\n",
    "\n",
    "Ordinary least squares (OLS) generates a model that purely minimizes the sum of squared residuals between the observed data and the predicted values. However OLS can run into trouble when multicollinearity is present because the OLS regression model will have an extremely large variance which in turn results in the model not performing well when predicting. A solution to this problematic behaviour is to perform regularization which in essence significantly lowers variance with a tradeoff of introducing a little bias. LASSO, Ridge, and Elastic Net Regression are all examples of performing regularization just with slight differences in the regularization/penalty terms they use.  \n",
    "\n",
    "$$L_{LASSO} = ||y - Xw||^{2} + \\lambda||w||_{1}$$\n",
    "LASSO regression implements regularization by introducing the L-1 norm of the weights. As is the purpose of regularization, the L-1 term penalizes large weights. Because LASSO uses the L1-norm, covariates that are not useful in predicting the observed data will have their corresponding weights set to 0 meaning that the LASSO regression model will ignore those covariates when making predictions. As such, LASSO regression is beneficial when there are a lot of covariates present that do not help meaningfully when making predictions. In the presence of multicollinearity, LASSO regression will eliminate most of these highly correlated covariates (set their corresponding coefficients to 0) and keep only one of the correlated covariates (by setting a larger coefficent). Intuitively, this is expected since the individual impact of highly correlated covariates cannot be distinguished, the rest become useless after learning the effect of one of the correlated covariates. This behaviour can be observed in the above LASSO regression model where many of the coefficients have been set to 0.\n",
    "\n",
    "$$L_{Ridge} = ||y - Xw||^{2} + \\lambda||w||^{2}_{2}$$\n",
    "The difference between Ridge regression and LASSO is that Ridge utilizes the squared L-2 norm instead of the L-1 norm. Unlike LASSO, ridge regression will not set any of the weights to 0 as a result of the squared L-2 norm becoming insignificant when weights are close to 0. Thus in the presence of covariates that do not provide much information, ridge regression will not eliminate these variables even in the presence of multicollinearity as can be observed in the above Ridge regression model coefficients. Hence, ridge regression performs well when most covariates have individual meaningful impact on the response variable.\n",
    "\n",
    "$$L_{E\\_net} = \\frac{||y - Xw||^{2}}{2n} + \\alpha(\\lambda_{1}||w||_{1} + (1-\\lambda_{1})||w||^{2}_{2})$$\n",
    "Elastic net regression is a combination between ridge and LASSO regression. It utilizes both the L-2 penalty term of ridge regression and the L-1 penalty term of LASSO regression. $\\lambda_{1}$ allows the model to tune whether the L-1 penalty is more significant or the L-2 penalty is more significant. In general, elastic net offers the flexibility of being able to reap the benefits of both ridge and LASSO regression.  \n",
    "\n",
    "These models don't provide an extremely accurate predictive performance so we cannot provide a fully coherent picture of mechanisms underlying the sudden volatility changes in the market to an extremely accurate degree. However, one can analyze the coefficients of the Elastic Net Regression Model to get a better understanding of which covariates truly matter. For example, judging by the coefficients of the following descriptors:  \n",
    "EMV: 0.688462552  \n",
    "Macro – Interest Rates EMV Tracker: -0.570739923  \n",
    "We can see that the values of these two descriptors are significant when predicting the future value of the VIX index. In particular one can say that the EMV value has a positive correlation with the future VIX value and the Interest Rates EMV Tracker has an inverse correlation with the future VIX value. One can interpret the latter as \"when the topic of interest rates becomes more active, the VIX index is more likely to drop\". As such, we can use these techniques to predict the future volatility of the market which can help investors decide when to be active in the stock market or when they should stay out of the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A: Loading the MVIX data\n",
    "Loading and pre-processing the MVIX dataset to return the monthly log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min return -0.4859671049122771\n",
      "max return 0.85258794502174\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import NormalDist\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def load_market_returns_dataset(file_path):\n",
    "    \"\"\" This function loads a pickle file given a file path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the pickle file\n",
    "\n",
    "    Output:\n",
    "    - (dict): A dictionary consisting the dataset content.\n",
    "    \"\"\"\n",
    "    data = pickle.load(open(file_path, \"rb\"))\n",
    "    # process the data to return the monthly log returns (x_t / x_t-1)\n",
    "    return np.log(data.values[1:]/data.values[0:-1])\n",
    "\n",
    "dataset_path = f\"./data/MVIX.pkl\"\n",
    "market_data = load_market_returns_dataset(dataset_path)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "min_return, max_return = market_data.min(), market_data.max()\n",
    "print('min return', min_return)\n",
    "print('max return', max_return)\n",
    "\n",
    "print(len(market_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B: Discretization\n",
    "Discretizing the dataset into a histogram with 48 bins of equal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies:\n",
      "[ 1.  1.  2.  4.  1.  5.  3.  3.  6. 15. 14. 14. 16. 26. 19. 28. 23. 30.\n",
      " 18. 18. 24. 18. 11.  7.  9.  8.  6.  3.  6.  5.  5.  3.  3.  0.  0.  0.\n",
      "  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.]\n",
      "\n",
      "Bins:\n",
      "[-0.486      -0.45810417 -0.43020833 -0.4023125  -0.37441667 -0.34652083\n",
      " -0.318625   -0.29072917 -0.26283333 -0.2349375  -0.20704167 -0.17914583\n",
      " -0.15125    -0.12335417 -0.09545833 -0.0675625  -0.03966667 -0.01177083\n",
      "  0.016125    0.04402083  0.07191667  0.0998125   0.12770833  0.15560417\n",
      "  0.1835      0.21139583  0.23929167  0.2671875   0.29508333  0.32297917\n",
      "  0.350875    0.37877083  0.40666667  0.4345625   0.46245833  0.49035417\n",
      "  0.51825     0.54614583  0.57404167  0.6019375   0.62983333  0.65772917\n",
      "  0.685625    0.71352083  0.74141667  0.7693125   0.79720833  0.82510417\n",
      "  0.853     ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequencies, bins, _ = plt.hist(x=market_data, bins=48, range=(-0.486, 0.853), color='#0504aa',\n",
    "                            rwidth=0.9)\n",
    "plt.grid(axis='y', alpha=0.8)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(ymax=80)\n",
    "plt.title('VIX Stock Price Histogram')\n",
    "\n",
    "print('Frequencies:')\n",
    "print(frequencies)\n",
    "\n",
    "print('\\nBins:')\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C: Objective function for GA Solver\n",
    "Implementation of the MoG objective function for the GA Solver to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py - The main file that contains the objective function for the GA solver to minimize and the code that runs the GA solver\n",
    "\n",
    "import numpy as np\n",
    "from genetic_algorithm import GeneticAlgorithm as ga\n",
    "from plot_func import plot_3d\n",
    "from statistics import NormalDist\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "# def normalize(arr):\n",
    "#     \"\"\" This function normalizes a vector so the sum of its elements equal 1.\n",
    "\n",
    "#     Args:\n",
    "#     - arr (ndarray (shape: (k, 1))): A kX1 vector consisting k probability mixtures all in the range [0, 1].\n",
    "\n",
    "#     Output:\n",
    "#     - (ndarray (shape: (k, 1))): A kX1 vector consisting k normalized probability mixtures.\n",
    "#     \"\"\"\n",
    "\n",
    "#     return arr / np.sum(arr)\n",
    "\n",
    "\n",
    "def f(X):\n",
    "    \"\"\" This function calculates the objective value.\n",
    "\n",
    "    Args:\n",
    "    - X (ndarray (shape: (3*k, 1))): A 3*kX1 vector consisting k sets of parameters for each gaussian component of the model (probability mixture, mean, standard deviation)\n",
    "                                     Every 3rd element starting from index 0 is a probability mixture for component i corresponding to step i\n",
    "                                     Every 3rd element starting from index 1 is a gaussian mean for component i corresponding to step i\n",
    "                                     Every 3rd element starting from index 2 is a gaussian std. deviation for component i corresponding to step i\n",
    "\n",
    "    Output:\n",
    "    - float: objective value\n",
    "    \"\"\"\n",
    "\n",
    "    n = 359  # number of observations of monthly VIX market prices\n",
    "\n",
    "    num_bins = 48   # number of bins of histogram\n",
    "    bin_width = (0.853 - (-0.486)) / num_bins    # width of each bin\n",
    "    # the starting edges of each bin\n",
    "    bins = np.array([-0.486, -0.45810417, -0.43020833, -0.4023125, -0.37441667, -0.34652083,\n",
    "                     -0.318625, -0.29072917, -0.26283333, -0.2349375, -0.20704167, -0.17914583,\n",
    "                     -0.15125, -0.12335417, -0.09545833, -0.0675625, -0.03966667, -0.01177083,\n",
    "                     0.016125,  0.04402083,  0.07191667,  0.0998125,  0.12770833,  0.15560417,\n",
    "                     0.1835,  0.21139583,  0.23929167,  0.2671875,  0.29508333,  0.32297917,\n",
    "                     0.350875,  0.37877083,  0.40666667,  0.4345625,  0.46245833,  0.49035417,\n",
    "                     0.51825,  0.54614583,  0.57404167,  0.6019375,  0.62983333,  0.65772917,\n",
    "                     0.685625,  0.71352083,  0.74141667,  0.7693125,  0.79720833,  0.82510417])\n",
    "    # the observed proportions of each bin\n",
    "    observed_proportions = np.array(\n",
    "        [1,  1,  2,  4,  1,  5,  3,  3,  6, 15, 14, 14, 16, 26, 19, 28, 23, 30,\n",
    "         18, 18, 24, 18, 11,  7,  9,  8,  6,  3,  6,  5,  5,  3,  3,  0,  0,  0,\n",
    "         0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  1]) / n\n",
    "\n",
    "    lagrangeMul = X[len(X) - 1]  # extract lagrange multiplier\n",
    "    theta = X[:len(X) - 1]      # extract all parameters of gaussian components\n",
    "\n",
    "    dim = len(theta)\n",
    "\n",
    "    assert dim % 3 == 0, f\"The parameter length should be a multiple of 3 for every prob mixture, mean, standard deviation. Got {dim % 3}\"\n",
    "\n",
    "    k = int((dim) // 3)  # the number of regimes\n",
    "\n",
    "    # extract all the probability mixtures from the paramaters\n",
    "    probability_mixtures = theta[0::3]\n",
    "    # extract all the means of the gaussian components from the paramaters\n",
    "    gaussian_means = theta[1::3]\n",
    "    # extract all the std deviations of the gaussian components from the paramaters\n",
    "    gaussian_std_deviations = theta[2::3]\n",
    "\n",
    "    assert len(\n",
    "        probability_mixtures) == k, f\"Number of prob mixtures should be {k}. Got {len(probability_mixtures)}\"\n",
    "    assert len(\n",
    "        gaussian_means) == k, f\"Number of prob mixtures should be {k}. Got {len(gaussian_means)}\"\n",
    "    assert len(\n",
    "        gaussian_std_deviations) == k, f\"Number of prob mixtures should be {k}. Got {len(gaussian_std_deviations)}\"\n",
    "\n",
    "    obj = 0\n",
    "\n",
    "    for j in range(num_bins):\n",
    "        if observed_proportions[j] == 0:\n",
    "            continue\n",
    "\n",
    "        # calculate p_j(theta)\n",
    "        expected_proportion = 0\n",
    "        for i in range(k):\n",
    "            norm = NormalDist(\n",
    "                mu=gaussian_means[i], sigma=gaussian_std_deviations[i])\n",
    "            expected_proportion += probability_mixtures[i] * (\n",
    "                norm.cdf(bins[j] + bin_width) - norm.cdf(bins[j]))\n",
    "        expOverObs = expected_proportion / observed_proportions[j]\n",
    "        obj += observed_proportions[j] * np.log(expOverObs)\n",
    "\n",
    "    obj = -2 * n * obj\n",
    "\n",
    "    return obj + (lagrangeMul * (np.absolute(np.sum(probability_mixtures) - 1)))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # The number of regimes of the model (int from [1, 5])\n",
    "    k = 3\n",
    "\n",
    "    # Probability mixture belongs to [0, 1] ([0.05, 1] is satisfactory since we don't want a probability mixture to be 0 since the corresponding regime would essentially vanish)\n",
    "    # Means of guassian distributions are unrestricted ([-1, 1] is satisfactory)\n",
    "    # Standard deviation of guassian distributions belong to (0, inf) ([0.0001, 5] is satisfactory)\n",
    "    thetaBound = np.array([[0.05, 1], [-1, 1], [0.0001, 5]]*k)\n",
    "    # Langrage multiplier should be greater than 0 ([1000, 10000] was found to be good bounds during testing)\n",
    "    lagrangeBound = np.array([1000, 10000])\n",
    "    varbound = np.append(thetaBound, [lagrangeBound], axis=0)\n",
    "\n",
    "    algorithm_param = {'max_num_iteration': 1000,\n",
    "                       'population_size': 100,\n",
    "                       'mutation_probability': 0.1,\n",
    "                       'elit_ratio': 0.01,\n",
    "                       'crossover_probability': 0.5,\n",
    "                       'parents_portion': 0.3,\n",
    "                       'crossover_type': 'uniform',\n",
    "                       'max_iteration_without_improv': None}\n",
    "\n",
    "    model = ga(function=f, dimension=(3 * k) + 1,\n",
    "               variable_type='real',\n",
    "               variable_boundaries=varbound,\n",
    "               algorithm_parameters=algorithm_param)\n",
    "\n",
    "    model.run()\n",
    "\n",
    "    # plot_3d(func=f, bounds=model.var_bound.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix D: Objective value Without Lagrangian\n",
    "Computes $l(\\theta^{*})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Objective value without lagrangian\n",
    "def objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations):\n",
    "    n = 359  # number of observations of monthly VIX market prices\n",
    "\n",
    "    num_bins = 48   # number of bins of histogram\n",
    "    bin_width = (0.853 - (-0.486)) / num_bins    # width of each bin\n",
    "    # the starting edges of each bin\n",
    "    bins = np.array([-0.486, -0.45810417, -0.43020833, -0.4023125, -0.37441667, -0.34652083,\n",
    "                     -0.318625, -0.29072917, -0.26283333, -0.2349375, -0.20704167, -0.17914583,\n",
    "                     -0.15125, -0.12335417, -0.09545833, -0.0675625, -0.03966667, -0.01177083,\n",
    "                     0.016125,  0.04402083,  0.07191667,  0.0998125,  0.12770833,  0.15560417,\n",
    "                     0.1835,  0.21139583,  0.23929167,  0.2671875,  0.29508333,  0.32297917,\n",
    "                     0.350875,  0.37877083,  0.40666667,  0.4345625,  0.46245833,  0.49035417,\n",
    "                     0.51825,  0.54614583,  0.57404167,  0.6019375,  0.62983333,  0.65772917,\n",
    "                     0.685625,  0.71352083,  0.74141667,  0.7693125,  0.79720833,  0.82510417])\n",
    "    # the observed proportions of each bin\n",
    "    observed_proportions = np.array(\n",
    "        [1,  1,  2,  4,  1,  5,  3,  3,  6, 15, 14, 14, 16, 26, 19, 28, 23, 30,\n",
    "         18, 18, 24, 18, 11,  7,  9,  8,  6,  3,  6,  5,  5,  3,  3,  0,  0,  0,\n",
    "         0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  0,  1]) / n\n",
    "\n",
    "    obj = 0\n",
    "\n",
    "    for j in range(num_bins):\n",
    "        if observed_proportions[j] == 0:\n",
    "            continue\n",
    "\n",
    "        # calculate p_j(theta)\n",
    "        expected_proportion = 0\n",
    "        for i in range(k):\n",
    "            norm = NormalDist(\n",
    "                mu=gaussian_means[i], sigma=gaussian_std_deviations[i])\n",
    "            expected_proportion += probability_mixtures[i] * (\n",
    "                norm.cdf(bins[j] + bin_width) - norm.cdf(bins[j]))\n",
    "        # if expected_proportion == 0:\n",
    "        #     print(probability_mixtures)\n",
    "        expOverObs = expected_proportion / observed_proportions[j]\n",
    "        obj += observed_proportions[j] * np.log(expOverObs)\n",
    "\n",
    "    obj = -2 * n * obj\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective value for k=1: 63.509964849722614\n",
      "\n",
      "Objective value for k=2: 45.95790972641437\n",
      "\n",
      "Objective value for k=3: 45.23124358006618\n",
      "\n",
      "Objective value for k=4: 47.17121457851432\n",
      "\n",
      "Objective value for k=5: 57.318537929529946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# objective value for k=1 regimes\n",
    "k = 1\n",
    "probability_mixtures = np.array([0.999981984])\n",
    "gaussian_means = np.array([-0.00188338263])\n",
    "gaussian_std_deviations = np.array([0.184790282])\n",
    "print(f'Objective value for k={k}:', objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations))\n",
    "print('')\n",
    "\n",
    "# objective value for k=2 regimes\n",
    "k = 2\n",
    "probability_mixtures = np.array([0.456548294, 0.543460468])\n",
    "gaussian_means = np.array([-0.0463689628, 0.0368542901])\n",
    "gaussian_std_deviations = np.array([0.111799602, 0.221456599])\n",
    "print(f'Objective value for k={k}:', objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations))\n",
    "print('')\n",
    "\n",
    "# objective value for k=3 regimes\n",
    "k = 3\n",
    "probability_mixtures = np.array([0.0608524264, 0.150573290, 0.788531818])\n",
    "gaussian_means = np.array([-0.0475877375, 0.125448479, -0.0216219810])\n",
    "gaussian_std_deviations = np.array([0.0561458361, 0.244038960, 0.161205895])\n",
    "print(f'Objective value for k={k}:', objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations))\n",
    "print('')\n",
    "\n",
    "# objective value for k=4 regimes\n",
    "k = 4\n",
    "probability_mixtures = np.array([1.32297376e-01, 2.07789476e-01, 8.23921126e-02, 5.77850799e-01])\n",
    "gaussian_means = np.array([3.71483817e-03, -8.04863627e-02, 2.22703097e-01, -1.27775847e-02])\n",
    "gaussian_std_deviations = np.array([2.73503104e-01, 1.07804156e-01, 1.51214993e-01, 1.48975944e-01])\n",
    "print(f'Objective value for k={k}:', objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations))\n",
    "print('')\n",
    "\n",
    "# objective value for k=5 regimes\n",
    "k = 5\n",
    "probability_mixtures = np.array([3.68164046e-01, 6.13773256e-02, 4.08659270e-01, 5.07813967e-02, 1.11934824e-01])\n",
    "gaussian_means = np.array([-2.67953999e-02, 1.42801042e-02, 2.50715933e-02, -7.53068450e-02, -7.68256687e-02])\n",
    "gaussian_std_deviations = np.array([9.31952077e-02, 1.92861910e-01, 2.40994405e-01, 1.95623567e-01, 3.23133329e-01])\n",
    "print(f'Objective value for k={k}:', objectiveVal(k, probability_mixtures, gaussian_means, gaussian_std_deviations))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E: Regime Present at Any Given Month\n",
    "Computes the regime present at any given month. The output shows the regimes that are present at any given month of the data. Notice that only 2 regimes are present even though the 3 regime model was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regimes present at each month (regime can range from 1-3):\n",
      "[3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 2 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3\n",
      " 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3\n",
      " 3 3 2 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 2 3 3 3 3 3 3 3 2 3 2 3 3 3 3 2 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Compute the regime that gives the maximum conditional probability of generating the given observation\n",
    "def mcp(x_t):\n",
    "    k = 3\n",
    "    probability_mixtures = np.array([0.0608524264, 0.150573290, 0.788531818])\n",
    "    gaussian_means = np.array([-0.0475877375, 0.125448479, -0.0216219810])\n",
    "    gaussian_std_deviations = np.array([0.0561458361, 0.244038960, 0.161205895])\n",
    "\n",
    "    cond_probability = np.zeros(shape=k)\n",
    "    totalProb = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        norm = NormalDist(\n",
    "                mu=gaussian_means[i], sigma=gaussian_std_deviations[i])\n",
    "        totalProb += probability_mixtures[i] * norm.pdf(x_t)\n",
    "\n",
    "    for i in range(k):\n",
    "        norm = NormalDist(\n",
    "                mu=gaussian_means[i], sigma=gaussian_std_deviations[i])\n",
    "        probOfRegime = probability_mixtures[i] * norm.pdf(x_t)\n",
    "        cond_probability[i] = probOfRegime / totalProb\n",
    "    \n",
    "    return np.argmax(cond_probability)\n",
    "\n",
    "mcp_map = np.vectorize(mcp)\n",
    "presentRegime = mcp_map(market_data) # calculate \n",
    "presentRegime = np.reshape(presentRegime, newshape=len(presentRegime))\n",
    "presentRegime = presentRegime + 1 # add 1 due to indexing\n",
    "\n",
    "print('Regimes present at each month (regime can range from 1-3):')\n",
    "print(presentRegime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix F: Transition Matrix\n",
    "The probability transition matrix that describes the probability of switching between regime 2 and 3. As noted above, regime 1 never appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability Transition Matrix between regimes 2 and 3:\n",
      "[[0.05       0.95      ]\n",
      " [0.05621302 0.94378698]]\n"
     ]
    }
   ],
   "source": [
    "transitionMatrix = np.zeros(shape=(2,2))\n",
    "\n",
    "# calculate probability transition matrix\n",
    "for present in [2, 3]:\n",
    "    frequency = 0   # the number of times the present regime shows up\n",
    "    numSame = 0  # the number of times the regime switches to 1 from the present regime\n",
    "    numSwitch = 0  # the number of times the regime switches to 2 from the present regime\n",
    "    for i in range(len(presentRegime) - 1):\n",
    "        if presentRegime[i] == present:\n",
    "            frequency += 1\n",
    "            if presentRegime[i+1] == present:\n",
    "                numSame += 1\n",
    "            else:\n",
    "                numSwitch += 1\n",
    "    if present == 2:\n",
    "        transitionMatrix[0, 0] = numSame / frequency\n",
    "        transitionMatrix[0, 1] = numSwitch / frequency\n",
    "    else:\n",
    "        transitionMatrix[1, 0] = numSwitch / frequency\n",
    "        transitionMatrix[1, 1] = numSame / frequency\n",
    "print('Probability Transition Matrix between regimes 2 and 3:')\n",
    "print(transitionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix G: Loading the EMV Dataset\n",
    "Loading the MEMV.pkl dataset to develop a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_EMV_dataset(file_path):\n",
    "    \"\"\" This function loads a pickle file given a file path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the pickle file\n",
    "\n",
    "    Output:\n",
    "    - (dict): A dictionary consisting the dataset content.\n",
    "    \"\"\"\n",
    "    data = pickle.load(open(file_path, \"rb\"))\n",
    "    # return np.log(data.values[1:]/data.values[0:-1])\n",
    "    return data\n",
    "\n",
    "dataset_path = f\"./data/MEMV.pkl\"\n",
    "market_data = load_EMV_dataset(dataset_path)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "data = market_data.to_numpy()\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:,-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c41c53919e67cd82e791ee8ce8faf15b48b5a44f132b567f4eac48319f52b8f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
