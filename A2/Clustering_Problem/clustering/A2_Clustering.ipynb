{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Component (Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CSCC11 - Introduction to Machine Learning, Fall 2021, Assignment 2\n",
    "M. Ataei\n",
    "\"\"\"\n",
    "import _pickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (2225, 9635)\n",
      "terms shape:  (9635, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_pickle_dataset(file_path):\n",
    "    \"\"\" This function loads a pickle file given a file path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path of the pickle file\n",
    "\n",
    "    Output:\n",
    "    - (dict): A dictionary consisting the dataset content.\n",
    "    \"\"\"\n",
    "    return pickle.load(open(file_path, \"rb\"))\n",
    "\n",
    "dataset_path = f\"../data/BBC_data.pkl\"\n",
    "BBC_data = load_pickle_dataset(dataset_path)\n",
    "\n",
    "data = BBC_data['data']\n",
    "terms = BBC_data['terms']\n",
    "labels = BBC_data['labels']\n",
    "\n",
    "print('data shape: ', data.shape)\n",
    "print('terms shape: ', terms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the BBC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of entries equal to 0 per document: 9506.112808988764\n",
      "\n",
      "Ten Most Common Terms:\n",
      "year \t| Num Occurences: 2830\n",
      "peopl \t| Num Occurences: 2044\n",
      "on \t| Num Occurences: 1838\n",
      "game \t| Num Occurences: 1640\n",
      "time \t| Num Occurences: 1487\n",
      "first \t| Num Occurences: 1283\n",
      "govern \t| Num Occurences: 1246\n",
      "go \t| Num Occurences: 1222\n",
      "world \t| Num Occurences: 1214\n",
      "get \t| Num Occurences: 1196\n",
      "\n",
      "Ten Least Common Terms:\n",
      "chagrin \t| Num Occurences: 3\n",
      "bse \t| Num Occurences: 3\n",
      "angelina \t| Num Occurences: 3\n",
      "revolt \t| Num Occurences: 3\n",
      "Â£117m \t| Num Occurences: 3\n",
      "culprit \t| Num Occurences: 3\n",
      "blister \t| Num Occurences: 3\n",
      "horizont \t| Num Occurences: 3\n",
      "julio \t| Num Occurences: 3\n",
      "chill \t| Num Occurences: 3\n",
      "\n",
      "Average word-frequency: 1.4849044892493741\n"
     ]
    }
   ],
   "source": [
    "# Print on average, the number of entries that are 0 for each vector\n",
    "def averageNumZeros(data):\n",
    "    numZero = np.count_nonzero(data == 0)\n",
    "    print('Average number of entries equal to 0 per document:', numZero / data.shape[0])\n",
    "\n",
    "averageNumZeros(data=data)\n",
    "print('')\n",
    "\n",
    "# Print the 10 most common and least common terms\n",
    "def tenMostAndLeastCommon(data, terms):\n",
    "    numTermOccurences = np.sum(data, axis=0)\n",
    "    sortedInd = np.argsort(numTermOccurences)\n",
    "    tenMost = sortedInd[-10:][::-1]\n",
    "    tenLeast = sortedInd[:10]\n",
    "\n",
    "    print('Ten Most Common Terms:')\n",
    "    for term in tenMost:\n",
    "        print(terms[term][0][0].rstrip('\\n'), '\\t| Num Occurences:', numTermOccurences[term])\n",
    "\n",
    "    print('')\n",
    "    print('Ten Least Common Terms:')\n",
    "    for term in tenLeast:\n",
    "        print(terms[term][0][0].rstrip('\\n'), '\\t| Num Occurences:', numTermOccurences[term])\n",
    "\n",
    "tenMostAndLeastCommon(data, terms)\n",
    "print('')\n",
    "\n",
    "# print the average value for word-frequencies\n",
    "def averageWordFreq(data):\n",
    "    totalOccurences = np.sum(data)\n",
    "    numNonZeroOccurences = np.count_nonzero(data != 0)\n",
    "    print('Average word-frequency:', totalOccurences / numNonZeroOccurences)\n",
    "\n",
    "averageWordFreq(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 Pg.7 Step 0: Questions 1-3\n",
    "\n",
    "1). On average, each document vector has 9506 entries set to 0. Considering the fact that each document vector has a total 9635 entries, that means the average document has most of its entries set to 0. So in conclusion, the document term vectors are sparse.  \n",
    "\n",
    "2).\n",
    "```\n",
    "The 10 most common terms are:  \n",
    "year \t| Num Occurences: 2830  \n",
    "peopl \t| Num Occurences: 2044  \n",
    "on \t| Num Occurences: 1838  \n",
    "game \t| Num Occurences: 1640  \n",
    "time \t| Num Occurences: 1487  \n",
    "first \t| Num Occurences: 1283  \n",
    "govern \t| Num Occurences: 1246  \n",
    "go \t| Num Occurences: 1222  \n",
    "world \t| Num Occurences: 1214  \n",
    "get \t| Num Occurences: 1196  \n",
    "\n",
    "The 10 least common terms are:  \n",
    "chagrin     | Num Occurences: 3  \n",
    "bse \t    | Num Occurences: 3  \n",
    "angelina    | Num Occurences: 3  \n",
    "revolt \t    | Num Occurences: 3  \n",
    "Â£117m \t    | Num Occurences: 3  \n",
    "culprit     | Num Occurences: 3  \n",
    "blister     | Num Occurences: 3  \n",
    "horizont    | Num Occurences: 3  \n",
    "julio \t    | Num Occurences: 3  \n",
    "chill \t    | Num Occurences: 3  \n",
    "```\n",
    "\n",
    "3). The average value for word-frequencies is roughly 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans.py\n",
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, init_centers):\n",
    "        \"\"\" This class represents the K-means model.\n",
    "\n",
    "        TODO: You will need to implement the methods of this class:\n",
    "        - train: ndarray, int -> ndarray\n",
    "\n",
    "        Implementation description will be provided under each method.\n",
    "\n",
    "        For the following:\n",
    "        - N: Number of samples.\n",
    "        - D: Dimension of input features.\n",
    "        - K: Number of centers.\n",
    "             NOTE: K > 1\n",
    "\n",
    "        Args:\n",
    "        - init_centers (ndarray (shape: (K, D))): A KxD matrix consisting K D-dimensional centers.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(init_centers.shape) == 2, f\"init_centers should be a KxD matrix. Got: {init_centers.shape}\"\n",
    "        (self.K, self.D) = init_centers.shape\n",
    "        assert self.K > 1, f\"There must be at least 2 clusters. Got: {self.K}\"\n",
    "\n",
    "        # Shape: K x D\n",
    "        self.centers = np.copy(init_centers)\n",
    "\n",
    "    def train(self, train_X, max_iterations=1000):\n",
    "        \"\"\" This method trains the K-means model.\n",
    "\n",
    "        NOTE: This method updates self.centers\n",
    "\n",
    "        The algorithm is the following:\n",
    "        - Assigns data points to the closest cluster center.\n",
    "        - Re-computes cluster centers based on the data points assigned to them.\n",
    "        - Update the labels array to contain the index of the cluster center each point is assigned to.\n",
    "        - Loop ends when the labels do not change from one iteration to the next. \n",
    "\n",
    "        Args:\n",
    "        - train_X (ndarray (shape: (N, D))): A NxD matrix consisting N D-dimensional input data.\n",
    "        - max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "        Output:\n",
    "        - labels (ndarray (shape: (N, 1))): A N-column vector consisting N labels of input data.\n",
    "        \"\"\"\n",
    "        assert len(train_X.shape) == 2 and train_X.shape[1] == self.D, f\"train_X should be a NxD matrix. Got: {train_X.shape}\"\n",
    "        assert max_iterations > 0, f\"max_iterations must be positive. Got: {max_iterations}\"\n",
    "        N = train_X.shape[0]\n",
    "\n",
    "        labels = np.empty(shape=(N, 1), dtype=np.long)\n",
    "        distances = np.empty(shape=(N, self.K))\n",
    "        for _ in range(max_iterations):\n",
    "            old_labels = labels\n",
    "\n",
    "            # ====================================================\n",
    "            # TODO: Implement your solution within the box\n",
    "\n",
    "            labels = np.empty(shape=(N, 1), dtype=np.long)\n",
    "\n",
    "            # Calculate the distance between each training input and each cluster center\n",
    "            # Assign each training input to the closest cluster center\n",
    "            for i in range(N):\n",
    "                for j in range(self.K):\n",
    "                    dif = train_X[i] - self.centers[j]\n",
    "                    distances[i, j] = np.dot(dif.T, dif)\n",
    "\n",
    "                labels[i] = np.argmin(distances[i])\n",
    "\n",
    "            # Update each cluster center based on the inputs assigned to it\n",
    "            for j in range(self.K):\n",
    "                newCenter = np.zeros(self.D)\n",
    "                numAssigned = 0\n",
    "                for i in range(N):\n",
    "                    if labels[i] == j:\n",
    "                        newCenter += train_X[i]\n",
    "                        numAssigned += 1\n",
    "                self.centers[j] = (newCenter / numAssigned)\n",
    "                \n",
    "\n",
    "            # ====================================================\n",
    "\n",
    "            # Check convergence\n",
    "            if np.allclose(old_labels, labels):\n",
    "                break\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 Pg.7 Step 1: Questions 1-3\n",
    "\n",
    "1). I cannot really figure out the topics the clusters represent. Every cluster has the same few words repeated many times such as: time, on, govern, tax, player. etc. A rough guess would be sports and politics.  \n",
    "\n",
    "2). Some factors that make clustering difficult are:  \n",
    "- Determining the optimal hyper-parameters for example, it is difficult to determine how many clusters a dataset should have.  \n",
    "- There is no guarantee that the algorithm will get close to the global optimum. This algorithm could very likely get trapped in a poor local minima.  \n",
    "- Initialization can heavily impact the results of clustering and poor initialization can lead to poor results\n",
    "- Problems with the input data such as outliers\n",
    "- With this BBC dataset, the inputs have a lot of features which means we have to deal with very high dimensional data\n",
    "- The input data as mentioned previously is also sparse\n",
    "\n",
    "3). If we have a lucky guess at the cluster centers then yes, we should expect better results since this algorithm is sensitive to initialization. Each step of the optimization will lower the objective function so if we start near the global optimum, we will converge near the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 Pg.8 Step 2: Questions 1-3\n",
    "\n",
    "1). The error of the objective function is much lower (around 600000 in step 1 down to 250).  \n",
    "\n",
    "2). Compared to step 1, the clusters have a much bigger variety of words. From observing each cluster, I would roughly say that some topics are:\n",
    "- Soccer (because of words like: match, win, defeat, final, names of soccer teams, midfield, etc)\n",
    "- Politics (because of words like: elect, nation, campaign, govern, name of many countries etc.)\n",
    "- Film Industry (because of words like: film, critic, award, director, hollywood, cinema, etc)\n",
    "\n",
    "3). I would consider this result better than Step 1 since there was more variety of words in each cluster and I could even slightly notice a difference between some clusters. As opposed to in Step 1 where seemingly every cluster repeated the same few words many, many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2 Pg.8 Step 3: Questions 1-3\n",
    "\n",
    "1). From my observation the topics are:\n",
    "- Technology (because of words like: media, microsoft, apple, software, etc.)\n",
    "- Politics specifically in Europe (because of words like: govern, chancellor, tax, elect, names of many European countries, etc.)\n",
    "- Sports, more specifically European soccer (because of words like: team, win, defeat, championship, captain, names of European soccer clubs, etc.)\n",
    "- Economy (because of words like: market, growth, finance, company, account, economist, etc. )\n",
    "- Film industry (because of words like: film, director, hollywood, award, actor, grammy, names of famous actors, etc.)  \n",
    "\n",
    "2). After pre-processing the documents by performing random-walk diffusion, the documents in a cluster have a stronger sense of similarity with eachother. This results in the K-Means algorithm producing more meaningful clusters that have very similar data points. This is evident as the objective error has been reduced to roughly 3.5  \n",
    "\n",
    "3). In order to produce meaningful results/clusters, high-dimensional sparse data should be pre-processed. Otherwise, the results will very likely turn out very poor and meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmm.py\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import cov\n",
    "from scipy.stats import multivariate_normal\n",
    "from functools import partial\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, init_centers):\n",
    "        \"\"\" This class represents the GMM model.\n",
    "\n",
    "        TODO: You will need to implement the methods of this class:\n",
    "        - _e_step: ndarray, ndarray -> ndarray\n",
    "        - _m_step: ndarray, ndarray -> None\n",
    "\n",
    "        Implementation description will be provided under each method.\n",
    "\n",
    "        For the following:\n",
    "        - N: Number of samples.\n",
    "        - D: Dimension of input features.\n",
    "        - K: Number of Gaussians.\n",
    "             NOTE: K > 1\n",
    "\n",
    "        Args:\n",
    "        - init_centers (ndarray (shape: (K, D))): A KxD matrix consisting K D-dimensional centers, each for a Gaussian.\n",
    "        \"\"\"\n",
    "        assert len(\n",
    "            init_centers.shape) == 2, f\"init_centers should be a KxD matrix. Got: {init_centers.shape}\"\n",
    "        (self.K, self.D) = init_centers.shape\n",
    "        assert self.K > 1, f\"There must be at least 2 clusters. Got: {self.K}\"\n",
    "\n",
    "        # Shape: K x D\n",
    "        self.centers = np.copy(init_centers)\n",
    "\n",
    "        # Shape: K x D x D\n",
    "        self.covariances = np.tile(np.eye(self.D), reps=(self.K, 1, 1))\n",
    "\n",
    "        # Shape: K x 1\n",
    "        self.mixture_proportions = np.ones(shape=(self.K, 1)) / self.K\n",
    "\n",
    "    def _e_step(self, train_X):\n",
    "        \"\"\" This method performs the E-step of the EM algorithm.\n",
    "\n",
    "        Args:\n",
    "        - train_X (ndarray (shape: (N, D))): A NxD matrix consisting N D-dimensional input data.\n",
    "\n",
    "        Output:\n",
    "        - probability_matrix_updated (ndarray (shape: (N, K))): A NxK matrix consisting N conditional probabilities of p(z_k|x_i) (i.e. the responsibilities).\n",
    "        \"\"\"\n",
    "        (N, D) = train_X.shape\n",
    "        probability_matrix = np.empty(shape=(N, self.K))\n",
    "\n",
    "        # ====================================================\n",
    "        # TODO: Implement your solution within the box\n",
    "\n",
    "        # Compute the responsibilities\n",
    "        for i in range(N):\n",
    "            for j in range(self.K):\n",
    "                numer = self.mixture_proportions[j] * multivariate_normal.pdf(train_X[i],\n",
    "                                                                              mean=self.centers[j], cov=self.covariances[j])\n",
    "                denom = 0\n",
    "                for k in range(self.K):\n",
    "                    denom += self.mixture_proportions[k] * multivariate_normal.pdf(train_X[i],\n",
    "                                                                                   mean=self.centers[k], cov=self.covariances[k])\n",
    "                probability_matrix[i, j] = numer / denom\n",
    "\n",
    "        # ====================================================\n",
    "\n",
    "        assert probability_matrix.shape == (\n",
    "            train_X.shape[0], self.K), f\"probability_matrix shape mismatch. Expected: {(train_X.shape[0], self.K)}. Got: {probability_matrix.shape}\"\n",
    "\n",
    "        return probability_matrix\n",
    "\n",
    "    def _m_step(self, train_X, probability_matrix):\n",
    "        \"\"\" This method performs the M-step of the EM algorithm.\n",
    "\n",
    "        NOTE: This method updates self.centers, self.covariances, and self.mixture_proportions\n",
    "\n",
    "        Args:\n",
    "        - train_X (ndarray (shape: (N, D))): A NxD matrix consisting N D-dimensional input data.\n",
    "        - probability_matrix (ndarray (shape: (N, K))): A NxK matrix consisting N conditional probabilities of p(z_k|x_i) (i.e. the responsibilities).\n",
    "\n",
    "        Output:\n",
    "        - centers (ndarray (shape: (K, D))): A KxD matrix consisting K D-dimensional means for each Gaussian component.\n",
    "        - covariances (ndarray (shape: (K, D, D))): A KxDxD tensor consisting K DxD covariance matrix for each Gaussian component.\n",
    "        - mixture_proportions (ndarray (shape: (K, 1))): A K-column vector consistent the mixture proportion for each Gaussian component.\n",
    "        \"\"\"\n",
    "        (N, D) = train_X.shape\n",
    "\n",
    "        centers = np.empty(shape=(self.K, self.D))\n",
    "        covariances = np.empty(shape=(self.K, self.D, self.D))\n",
    "        mixture_proportions = np.empty(shape=(self.K, 1))\n",
    "        # ====================================================\n",
    "        # TODO: Implement your solution within the box\n",
    "\n",
    "        for j in range(self.K):\n",
    "            sumProb = np.sum(probability_matrix[:, j])\n",
    "\n",
    "            # compute the mixture probability\n",
    "            mixture_proportions[j] = sumProb / N\n",
    "\n",
    "            # compute the mean for the Guassian distribution\n",
    "            centers[j] = np.sum(\n",
    "                train_X * probability_matrix[:, j].reshape(len(train_X), 1), axis=0) / sumProb\n",
    "\n",
    "            # compute the covariance matrix\n",
    "            cov = np.zeros(shape=(D, D))\n",
    "            for i in range(N):\n",
    "                dif = np.reshape(train_X[i] - centers[j], (D, 1))\n",
    "                cov += probability_matrix[i, j] * np.dot(dif, dif.T)\n",
    "            covariances[j] = cov / sumProb\n",
    "\n",
    "        #\n",
    "        # ====================================================\n",
    "\n",
    "        assert centers.shape == (\n",
    "            self.K, self.D), f\"centers shape mismatch. Expected: {(self.K, self.D)}. Got: {centers.shape}\"\n",
    "        assert covariances.shape == (\n",
    "            self.K, self.D, self.D), f\"covariances shape mismatch. Expected: {(self.K, self.D, self.D)}. Got: {covariances.shape}\"\n",
    "        assert mixture_proportions.shape == (\n",
    "            self.K, 1), f\"mixture_proportions shape mismatch. Expected: {(self.K, 1)}. Got: {mixture_proportions.shape}\"\n",
    "\n",
    "        return centers, covariances, mixture_proportions\n",
    "\n",
    "    def train(self, train_X, max_iterations=1000):\n",
    "        \"\"\" This method trains the GMM model using EM algorithm.\n",
    "\n",
    "        NOTE: This method updates self.centers, self.covariances, and self.mixture_proportions\n",
    "\n",
    "        Args:\n",
    "        - train_X (ndarray (shape: (N, D))): A NxD matrix consisting N D-dimensional input data.\n",
    "        - max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "        Output:\n",
    "        - labels (ndarray (shape: (N, 1))): A N-column vector consisting N labels of input data.\n",
    "        \"\"\"\n",
    "        assert len(\n",
    "            train_X.shape) == 2 and train_X.shape[1] == self.D, f\"train_X should be a NxD matrix. Got: {train_X.shape}\"\n",
    "        assert max_iterations > 0, f\"max_iterations must be positive. Got: {max_iterations}\"\n",
    "        N = train_X.shape[0]\n",
    "\n",
    "        e_step = partial(self._e_step, train_X=train_X)\n",
    "        m_step = partial(self._m_step, train_X=train_X)\n",
    "\n",
    "        labels = np.empty(shape=(N, 1), dtype=np.long)\n",
    "        for _ in range(max_iterations):\n",
    "            old_labels = labels\n",
    "            # E-Step\n",
    "            probability_matrix = e_step()\n",
    "\n",
    "            # Reassign labels\n",
    "            labels = np.argmax(probability_matrix, axis=1).reshape((N, 1))\n",
    "\n",
    "            # Check convergence\n",
    "            if np.allclose(old_labels, labels):\n",
    "                break\n",
    "\n",
    "            # M-Step\n",
    "            self.centers, self.covariances, self.mixture_proportions = m_step(\n",
    "                probability_matrix=probability_matrix)\n",
    "\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM Questions\n",
    "For some of the datasets, GMM returns overlapping clusters whereas K-Means always returns strictly partitioned (non-overlapping) clusters. Another difference is that all the K-Means clusters have more or less a circular shape where as GMM clusters can vary with circular and elliptical shapes.  \n",
    "\n",
    "The reason for this is because K-Means is a distance based model and it focuses on minimizing the Euclidean distance between the cluster centers and their respectively assigned data points. Hence, it is clear why the K-Means clusters do not overlap because if there was overlap, then the distance between between cluster centers and their assigned data points are clearly not minimized.  \n",
    "However, in GMM we are focusing on figuring out the probability of a data point belonging to a cluster. We assume the data points to be sampled from K Guassian distributions and we are trying to estimate the parameters of these Guassian components. GMM also allows us to express some prior beliefs of the fraction of data assigned to each Guassian distribution. Thus GMM clusters can adapt to return overlapping and elliptical shaped clusters. The extra adaptability is why GMM seems to do better on some of these datasets.\n",
    "\n",
    "Regarding the task of document clustering, because the BBC dataset has such high dimensional data, we could perform pre-processing on the data in order to run GMM for this task. For example, we could perform dimensionality reduction on the data by avoiding unecessary dimensions since as we saw previously, the BBC dataset has sparse data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c41c53919e67cd82e791ee8ce8faf15b48b5a44f132b567f4eac48319f52b8f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
